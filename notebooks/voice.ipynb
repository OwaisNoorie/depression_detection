{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\depression_detect\\venv\\lib\\site-packages\\librosa\\core\\pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete! Saved to D:/depression_detection/preprocessed_data/cremad_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CREMA-D dataset\n",
    "cremad_dir = \"D:/depression_detect/datasets/cremad\"\n",
    "\n",
    "# Function to extract audio features\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "        mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)\n",
    "        return np.hstack([mfccs, chroma, mel])  # Combine all features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process all audio files in the dataset\n",
    "features, labels, file_names = [], [], []\n",
    "for file in os.listdir(cremad_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        label = 1 if \"SAD\" in file or \"DIS\" in file else 0  # Depressed = 1, Not Depressed = 0\n",
    "        file_path = os.path.join(cremad_dir, file)\n",
    "        feature_vector = extract_features(file_path)\n",
    "        if feature_vector is not None:\n",
    "            features.append(feature_vector)\n",
    "            labels.append(label)\n",
    "            file_names.append(file)  # Save file names for reference\n",
    "\n",
    "# Convert extracted features into a DataFrame\n",
    "feature_df = pd.DataFrame(features)\n",
    "feature_df['label'] = labels\n",
    "feature_df['file_name'] = file_names  # Add file names for tracking\n",
    "\n",
    "# Save extracted features to a CSV file\n",
    "save_path = \"D:/depression_detection/preprocessed_data/cremad_features.csv\"\n",
    "feature_df.to_csv(save_path, index=False)\n",
    "print(f\"Feature extraction complete! Saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ cremad_features.csv exists!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"D:/depression_detect/preprocessed_data/cremad_features.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"✅ cremad_features.csv exists!\")\n",
    "else:\n",
    "    print(\"❌ cremad_features.csv is missing!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ `cremad_features.csv` successfully created with labels!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"D:/depression_detect/datasets/cremad/\"\n",
    "\n",
    "# Get all audio files\n",
    "audio_files = [f for f in os.listdir(dataset_path) if f.endswith('.wav')]\n",
    "\n",
    "# Define emotion-to-label mapping\n",
    "depressed_labels = [\"SAD\", \"ANG\"]  # Depressed emotions → Label 1\n",
    "non_depressed_labels = [\"HAP\", \"NEU\"]  # Non-depressed emotions → Label 0\n",
    "\n",
    "# Initialize list for features\n",
    "data = []\n",
    "\n",
    "# Extract features and labels\n",
    "for file in audio_files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "\n",
    "    # Extract emotion from filename (Example: \"1001_IEO_SAD_HI.wav\" → \"SAD\")\n",
    "    emotion = file.split(\"_\")[2]\n",
    "\n",
    "    # Assign label (1 for depressed, 0 for non-depressed)\n",
    "    label = 1 if emotion in depressed_labels else 0\n",
    "\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)  # Take mean over time\n",
    "\n",
    "    # Append to list\n",
    "    data.append([file_path] + mfccs_mean.tolist() + [label])\n",
    "\n",
    "# Create DataFrame\n",
    "columns = ['file_path'] + [f'mfcc_{i+1}' for i in range(13)] + ['label']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "save_path = \"D:/depression_detect/preprocessed_data/cremad_features.csv\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)  # Create directory if missing\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"✅ `cremad_features.csv` successfully created with labels!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0           1          2          3          4          5  \\\n",
      "0 -306.02740   92.670235   8.491313  23.965403   7.477993  -5.759456   \n",
      "1 -346.39963   95.839120  10.516283  31.619215  15.872088  -6.845447   \n",
      "2 -321.42026   94.760910   8.155397  23.323242  11.719156  -7.116331   \n",
      "3 -303.30374   92.528890   4.231231  27.970133  10.869824 -11.878345   \n",
      "4 -335.49590  100.393310   9.384935  30.160906  11.466775  -3.333670   \n",
      "\n",
      "           6         7         8          9  ...       145       146  \\\n",
      "0 -11.883088 -9.676736 -3.996747 -13.352563  ...  0.002250  0.003137   \n",
      "1  -6.629935 -4.978727 -5.310654 -10.283518  ...  0.000678  0.000941   \n",
      "2  -8.534803 -4.996966 -4.994400 -13.706510  ...  0.004759  0.011868   \n",
      "3 -10.095112 -7.149731 -7.651760 -17.085900  ...  0.008727  0.012495   \n",
      "4  -8.350987 -9.757345 -6.079327 -12.109532  ...  0.000724  0.000856   \n",
      "\n",
      "        147       148       149       150       151       152  label  \\\n",
      "0  0.003351  0.002825  0.002283  0.002875  0.003217  0.002537      0   \n",
      "1  0.001306  0.001039  0.001349  0.001003  0.001274  0.001167      1   \n",
      "2  0.017361  0.012410  0.019017  0.013379  0.010257  0.008362      0   \n",
      "3  0.010875  0.005356  0.006022  0.003931  0.003002  0.003396      0   \n",
      "4  0.000796  0.000431  0.000584  0.000571  0.000778  0.000657      0   \n",
      "\n",
      "             file_name  \n",
      "0  1001_DFA_ANG_XX.wav  \n",
      "1  1001_DFA_DIS_XX.wav  \n",
      "2  1001_DFA_FEA_XX.wav  \n",
      "3  1001_DFA_HAP_XX.wav  \n",
      "4  1001_DFA_NEU_XX.wav  \n",
      "\n",
      "[5 rows x 155 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7442 entries, 0 to 7441\n",
      "Columns: 155 entries, 0 to file_name\n",
      "dtypes: float64(153), int64(1), object(1)\n",
      "memory usage: 8.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"D:/depression_detection/preprocessed_data/cremad_features.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(df.head())  # Show first few rows\n",
    "print(df.info())  # Check column data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names in CSV: Index(['file_path', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6',\n",
      "       'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12',\n",
      "       'mfcc_13', 'label'],\n",
      "      dtype='object')\n",
      "✅ Model Accuracy: 0.8522\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1188   47]\n",
      " [ 173   81]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92      1235\n",
      "           1       0.63      0.32      0.42       254\n",
      "\n",
      "    accuracy                           0.85      1489\n",
      "   macro avg       0.75      0.64      0.67      1489\n",
      "weighted avg       0.83      0.85      0.83      1489\n",
      "\n",
      "✅ Model saved to: D:/depression_detect/models/random_forest_cremad.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load extracted features\n",
    "data_path = \"D:/depression_detect/preprocessed_data/cremad_features.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Print column names to check if 'file_path' exists\n",
    "print(\"Column Names in CSV:\", df.columns)\n",
    "\n",
    "# If 'file_path' exists, extract labels, else notify user\n",
    "if 'file_path' in df.columns:\n",
    "    import os\n",
    "    \n",
    "    def extract_label(file_path):\n",
    "        filename = os.path.basename(file_path)\n",
    "        emotion = filename.split('_')[2]  # Example: \"1001_DFA_**ANG**_XX.wav\"\n",
    "        return 1 if emotion == \"SAD\" else 0  # Depressed = 1, Not Depressed = 0\n",
    "    \n",
    "    df['label'] = df['file_path'].apply(extract_label)\n",
    "    df = df.drop(columns=['file_path'])  # Drop file_path after extracting labels\n",
    "else:\n",
    "    print(\"❌ Warning: 'file_path' column is missing! Add labels manually.\")\n",
    "    exit()  # Stop execution because labels are required\n",
    "\n",
    "# Feature and label split\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"✅ Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix & classification report\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"D:/depression_detect/models/random_forest_cremad.pkl\"\n",
    "joblib.dump(rf_model, model_path)\n",
    "print(f\"✅ Model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: Index(['mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7',\n",
      "       'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in DataFrame:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7',\n",
      "       'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'y': [0 1]\n",
      "Class distribution:\n",
      " label\n",
      "0    6171\n",
      "1    1271\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in 'y':\", y.unique())\n",
    "print(\"Class distribution:\\n\", y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5     mfcc_6  \\\n",
      "0 -306.027405   92.670235   8.491313  23.965403   7.477993  -5.759456   \n",
      "1 -346.399628   95.839119  10.516283  31.619215  15.872088  -6.845447   \n",
      "2 -321.420258   94.760910   8.155397  23.323242  11.719156  -7.116331   \n",
      "3 -303.303741   92.528893   4.231231  27.970133  10.869824 -11.878345   \n",
      "4 -335.495911  100.393311   9.384935  30.160906  11.466775  -3.333670   \n",
      "\n",
      "      mfcc_7    mfcc_8    mfcc_9    mfcc_10   mfcc_11   mfcc_12   mfcc_13  \\\n",
      "0 -11.883088 -9.676736 -3.996747 -13.352563  0.408197 -9.709486 -6.127124   \n",
      "1  -6.629935 -4.978727 -5.310654 -10.283518 -2.534367 -7.255390 -6.153906   \n",
      "2  -8.534803 -4.996966 -4.994400 -13.706510 -3.357414 -8.454173 -6.561941   \n",
      "3 -10.095112 -7.149731 -7.651760 -17.085899 -0.201025 -8.867324 -9.357766   \n",
      "4  -8.350987 -9.757345 -6.079327 -12.109532  1.537681 -9.795646 -3.472060   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SMOTE applied! New class distribution:\n",
      "label\n",
      "1    4900\n",
      "0    4900\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Model Accuracy after SMOTE: 0.7367\n",
      "\n",
      "Confusion Matrix:\n",
      " [[764 192]\n",
      " [324 680]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75       956\n",
      "           1       0.78      0.68      0.72      1004\n",
      "\n",
      "    accuracy                           0.74      1960\n",
      "   macro avg       0.74      0.74      0.74      1960\n",
      "weighted avg       0.74      0.74      0.74      1960\n",
      "\n",
      "\n",
      "✅ Model saved to: D:/depression_detect/models/random_forest_cremad_smote.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"D:/depression_detect/preprocessed_data/cremad_features.csv\")\n",
    "\n",
    "# Step 1: Extract Labels from File Paths\n",
    "def extract_label(file_path):\n",
    "    \"\"\"Extracts label from file name based on emotion code in filename.\"\"\"\n",
    "    filename = os.path.basename(file_path)  # Extracts '1001_DFA_ANG.wav'\n",
    "    emotion = filename.split('_')[-2]  # Extracts 'ANG' (corrected position)\n",
    "\n",
    "    # Map emotions to binary labels (Modify based on dataset specifics)\n",
    "    depressed_emotions = [\"ANG\", \"SAD\", \"FEA\", \"DIS\"]  # Depressed (1)\n",
    "    return 1 if emotion in depressed_emotions else 0\n",
    "\n",
    "# Step 2: Generate Labels if Missing\n",
    "if 'label' not in df.columns:\n",
    "    if 'file_path' in df.columns:\n",
    "        df['label'] = df['file_path'].apply(extract_label)\n",
    "        \n",
    "    else:\n",
    "        raise KeyError(\"❌ ERROR: 'file_path' column is missing, can't extract labels!\")\n",
    "\n",
    "# **Drop `file_path` column to prevent string-to-float error**\n",
    "if 'file_path' in df.columns:\n",
    "    df.drop(columns=['file_path'], inplace=True)\n",
    "\n",
    "# Step 3: Split Features & Labels\n",
    "X = df.drop(columns=['label'])  # Features\n",
    "y = df['label']  # Labels\n",
    "\n",
    "# Step 4: Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "print(\"✅ SMOTE applied! New class distribution:\")\n",
    "print(y_resampled.value_counts())\n",
    "\n",
    "# Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train Random Forest Model\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate Model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n✅ Model Accuracy after SMOTE: {accuracy:.4f}\\n\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 8: Save Model\n",
    "model_path = \"D:/depression_detect/models/random_forest_cremad_smote.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"\\n✅ Model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (7442, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X.shape)  # Should be (samples, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7',\n",
      "       'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # This will show all columns in your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['file_path'], errors='ignore')  # Ignore if not present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after fixing: (7442, 13)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['label']).values\n",
    "print(\"Shape of X after fixing:\", X.shape)  # Should now be (samples, 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], 13, 1)  # Now it should work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Shapes: X_train=(5953, 13, 1), X_test=(1489, 13, 1), y_train=(5953, 2), y_test=(1489, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"D:/depression_detect/preprocessed_data/cremad_features.csv\")\n",
    "\n",
    "# Extract Labels from File Paths\n",
    "def extract_label(file_path):\n",
    "    \"\"\"Extracts label from file name based on emotion code in filename.\"\"\"\n",
    "    filename = os.path.basename(file_path)  \n",
    "    emotion = filename.split('_')[-1].split('.')[0]  \n",
    "\n",
    "    # Depression-related emotions\n",
    "    depressed_emotions = [\"ANG\", \"SAD\", \"FEA\", \"DIS\"]  \n",
    "    return 1 if emotion in depressed_emotions else 0\n",
    "\n",
    "# Ensure labels exist\n",
    "if 'label' not in df.columns:\n",
    "    df['label'] = df['file_path'].apply(extract_label)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "df.drop(columns=['file_path'], inplace=True, errors='ignore')\n",
    "\n",
    "# Extract Features and Labels\n",
    "X = df.drop(columns=['label']).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize Features (before reshaping)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape for LSTM: (samples, timesteps, features)\n",
    "timesteps = X.shape[1]  # Each feature column becomes a timestep\n",
    "X = X.reshape(X.shape[0], timesteps, 1)  \n",
    "\n",
    "# Convert labels to categorical (binary classification)\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"✅ Final Shapes: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\depression_detect\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m12,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,074</span> (183.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m47,074\u001b[0m (183.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,946</span> (183.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,946\u001b[0m (183.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the LSTM + CNN Model\n",
    "model = Sequential([\n",
    "    # Convolutional Layer for feature extraction\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(13, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    \n",
    "    # LSTM Layer for sequence learning\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')  # Binary classification (depressed/not depressed)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.6447 - loss: 0.6461 - val_accuracy: 0.6964 - val_loss: 0.6022\n",
      "Epoch 2/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.6934 - loss: 0.6003 - val_accuracy: 0.7199 - val_loss: 0.5614\n",
      "Epoch 3/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7099 - loss: 0.5831 - val_accuracy: 0.7334 - val_loss: 0.5528\n",
      "Epoch 4/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7164 - loss: 0.5845 - val_accuracy: 0.7173 - val_loss: 0.5544\n",
      "Epoch 5/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7294 - loss: 0.5580 - val_accuracy: 0.7005 - val_loss: 0.5688\n",
      "Epoch 6/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7153 - loss: 0.5760 - val_accuracy: 0.7267 - val_loss: 0.5629\n",
      "Epoch 7/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7365 - loss: 0.5490 - val_accuracy: 0.7240 - val_loss: 0.5473\n",
      "Epoch 8/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7330 - loss: 0.5546 - val_accuracy: 0.7267 - val_loss: 0.5439\n",
      "Epoch 9/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7393 - loss: 0.5408 - val_accuracy: 0.7233 - val_loss: 0.5569\n",
      "Epoch 10/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7289 - loss: 0.5523 - val_accuracy: 0.7307 - val_loss: 0.5452\n",
      "Epoch 11/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7435 - loss: 0.5354 - val_accuracy: 0.7381 - val_loss: 0.5472\n",
      "Epoch 12/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7503 - loss: 0.5192 - val_accuracy: 0.7246 - val_loss: 0.5463\n",
      "Epoch 13/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7547 - loss: 0.5147 - val_accuracy: 0.7253 - val_loss: 0.5580\n",
      "Epoch 14/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7473 - loss: 0.5102 - val_accuracy: 0.7287 - val_loss: 0.5635\n",
      "Epoch 15/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7511 - loss: 0.4997 - val_accuracy: 0.7307 - val_loss: 0.5822\n",
      "Epoch 16/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7603 - loss: 0.4949 - val_accuracy: 0.7340 - val_loss: 0.5694\n",
      "Epoch 17/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7770 - loss: 0.4643 - val_accuracy: 0.7253 - val_loss: 0.5818\n",
      "Epoch 18/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7675 - loss: 0.4648 - val_accuracy: 0.7199 - val_loss: 0.6006\n",
      "Epoch 19/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7878 - loss: 0.4407 - val_accuracy: 0.7280 - val_loss: 0.6141\n",
      "Epoch 20/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7844 - loss: 0.4269 - val_accuracy: 0.7126 - val_loss: 0.6283\n",
      "Epoch 21/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8057 - loss: 0.4008 - val_accuracy: 0.7320 - val_loss: 0.6464\n",
      "Epoch 22/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8145 - loss: 0.3894 - val_accuracy: 0.7146 - val_loss: 0.6865\n",
      "Epoch 23/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8115 - loss: 0.3697 - val_accuracy: 0.7072 - val_loss: 0.6873\n",
      "Epoch 24/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8180 - loss: 0.3697 - val_accuracy: 0.6991 - val_loss: 0.7479\n",
      "Epoch 25/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8385 - loss: 0.3406 - val_accuracy: 0.6958 - val_loss: 0.7670\n",
      "Epoch 26/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8149 - loss: 0.3772 - val_accuracy: 0.7038 - val_loss: 0.7988\n",
      "Epoch 27/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8399 - loss: 0.3282 - val_accuracy: 0.7072 - val_loss: 0.8355\n",
      "Epoch 28/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8696 - loss: 0.2869 - val_accuracy: 0.6931 - val_loss: 0.8665\n",
      "Epoch 29/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8683 - loss: 0.2825 - val_accuracy: 0.6736 - val_loss: 0.8565\n",
      "Epoch 30/30\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.8822 - loss: 0.2694 - val_accuracy: 0.6944 - val_loss: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=30, batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"D:/depression_detect/models/lstm_cnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8916 - loss: 0.2415 - val_accuracy: 0.6823 - val_loss: 0.9815 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8840 - loss: 0.2421 - val_accuracy: 0.6870 - val_loss: 1.0403 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8537 - loss: 0.3529 - val_accuracy: 0.6991 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8814 - loss: 0.2703 - val_accuracy: 0.6877 - val_loss: 1.0100 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9079 - loss: 0.2142 - val_accuracy: 0.6749 - val_loss: 1.1472 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m181/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9115 - loss: 0.2120\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9115 - loss: 0.2118 - val_accuracy: 0.6729 - val_loss: 1.1796 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9314 - loss: 0.1677 - val_accuracy: 0.6844 - val_loss: 1.2923 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9121 - loss: 0.2231 - val_accuracy: 0.6857 - val_loss: 1.2662 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Early Stopping: Stop training if validation accuracy stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Reduce Learning Rate if validation loss doesn't improve\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "# Train again with callbacks\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=50, batch_size=32, \n",
    "                    callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Save the improved model\n",
    "model.save(\"D:/depression_detect/models/lstm_cnn_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\depression_detect\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.5784 - loss: 5.7587 - val_accuracy: 0.6662 - val_loss: 2.4861 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.6364 - loss: 2.1675 - val_accuracy: 0.6662 - val_loss: 1.3923 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6617 - loss: 1.2928 - val_accuracy: 0.6662 - val_loss: 1.0304 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.6666 - loss: 0.9667 - val_accuracy: 0.6662 - val_loss: 0.8532 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.6834 - loss: 0.8152 - val_accuracy: 0.6662 - val_loss: 0.7650 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6771 - loss: 0.7337 - val_accuracy: 0.6736 - val_loss: 0.7006 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - accuracy: 0.6933 - loss: 0.6905 - val_accuracy: 0.6803 - val_loss: 0.6723 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6808 - loss: 0.6765 - val_accuracy: 0.7005 - val_loss: 0.6416 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.6825 - loss: 0.6588 - val_accuracy: 0.7045 - val_loss: 0.6286 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.6927 - loss: 0.6425 - val_accuracy: 0.7045 - val_loss: 0.6166 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.7113 - loss: 0.6181 - val_accuracy: 0.6763 - val_loss: 0.6606 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m93/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.6905 - loss: 0.6389\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.6905 - loss: 0.6388 - val_accuracy: 0.6911 - val_loss: 0.6229 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7069 - loss: 0.6235 - val_accuracy: 0.7166 - val_loss: 0.5997 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7009 - loss: 0.6202 - val_accuracy: 0.7213 - val_loss: 0.5946 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.6951 - loss: 0.6123 - val_accuracy: 0.7058 - val_loss: 0.5961 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m93/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7107 - loss: 0.6082\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7105 - loss: 0.6083 - val_accuracy: 0.6991 - val_loss: 0.6029 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7054 - loss: 0.6037 - val_accuracy: 0.7139 - val_loss: 0.5872 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7152 - loss: 0.5935 - val_accuracy: 0.7119 - val_loss: 0.5877 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.7047 - loss: 0.6072 - val_accuracy: 0.7186 - val_loss: 0.5847 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7198 - loss: 0.5942 - val_accuracy: 0.7220 - val_loss: 0.5827 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.7070 - loss: 0.6013 - val_accuracy: 0.7072 - val_loss: 0.5823 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7116 - loss: 0.5951 - val_accuracy: 0.7112 - val_loss: 0.5807 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7006 - loss: 0.6064 - val_accuracy: 0.7112 - val_loss: 0.5801 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.7046 - loss: 0.6049 - val_accuracy: 0.7253 - val_loss: 0.5730 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7162 - loss: 0.5916 - val_accuracy: 0.7287 - val_loss: 0.5759 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7009 - loss: 0.6043 - val_accuracy: 0.7273 - val_loss: 0.5725 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7106 - loss: 0.5945 - val_accuracy: 0.7273 - val_loss: 0.5730 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m93/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7177 - loss: 0.5840\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7175 - loss: 0.5842 - val_accuracy: 0.7085 - val_loss: 0.5824 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.7045 - loss: 0.6050 - val_accuracy: 0.7126 - val_loss: 0.5826 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define Optimized LSTM Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01)), input_shape=(13, 1)),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Bidirectional(LSTM(64, return_sequences=False, kernel_regularizer=l2(0.01))),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(2, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)  # Gradient Clipping\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_lr=1e-6)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50, batch_size=64,  # Increased batch size\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the Model\n",
    "model.save('D:/depression_detection/models/optimized_lstm_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X and y contain your features and labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Unique labels in y:\", np.unique(y))  # This should print 6 unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_flat, y_flat)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert `y_resampled` back to one-hot encoding\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m y_resampled \u001b[38;5;241m=\u001b[39m to_categorical(y_resampled, num_classes\u001b[38;5;241m=\u001b[39m\u001b[43mnum_classes\u001b[49m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Split into train/val sets\u001b[39;00m\n\u001b[0;32m     17\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     18\u001b[0m     X_resampled, y_resampled, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_resampled\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Reshape `X` to 2D for SMOTE\n",
    "X_flat = X.reshape(X.shape[0], -1)  # Convert (samples, timesteps, features) → (samples, features)\n",
    "y_flat = np.argmax(y, axis=1)  # Convert one-hot encoded `y` back to categorical labels\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_flat, y_flat)\n",
    "\n",
    "# Convert `y_resampled` back to one-hot encoding\n",
    "y_resampled = to_categorical(y_resampled, num_classes=num_classes)\n",
    "\n",
    "# Split into train/val sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Reshape `X_train` & `X_val` back to 3D (for CNN + LSTM)\n",
    "X_train = X_train.reshape(-1, X.shape[1], X.shape[2])\n",
    "X_val = X_val.reshape(-1, X.shape[1], X.shape[2])\n",
    "\n",
    "# Print Shapes to Verify\n",
    "print(\"X_train shape:\", X_train.shape)  # (samples, timesteps, features)\n",
    "print(\"y_train shape:\", y_train.shape)  # (samples, num_classes)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\depression_detect\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 26\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregularizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m l2\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Convolutional Layers (Feature Extraction)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     Conv1D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39ml2(\u001b[38;5;241m0.001\u001b[39m), input_shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])),\n\u001b[0;32m     10\u001b[0m     BatchNormalization(),\n\u001b[0;32m     11\u001b[0m     MaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m     Conv1D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39ml2(\u001b[38;5;241m0.001\u001b[39m)),\n\u001b[0;32m     14\u001b[0m     BatchNormalization(),\n\u001b[0;32m     15\u001b[0m     MaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# LSTM Layer\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     Bidirectional(LSTM(\u001b[38;5;241m128\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)),\n\u001b[0;32m     19\u001b[0m     Bidirectional(LSTM(\u001b[38;5;241m64\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)),\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Fully Connected Layers\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     23\u001b[0m     Dropout(\u001b[38;5;241m0.4\u001b[39m),\n\u001b[0;32m     24\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     25\u001b[0m     Dropout(\u001b[38;5;241m0.4\u001b[39m),\n\u001b[1;32m---> 26\u001b[0m     Dense(\u001b[43mnum_classes\u001b[49m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Make sure num_classes = 6\u001b[39;00m\n\u001b[0;32m     27\u001b[0m ])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Compile Model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "# Define Model: CNN + BiLSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    # Convolutional Layers (Feature Extraction)\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    # LSTM Layer\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "    Bidirectional(LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.2)),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(num_classes, activation='softmax')  # Make sure num_classes = 6\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "optimizer = Adam(learning_rate=5e-4)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.7135 - loss: 0.5854 - val_accuracy: 0.7025 - val_loss: 0.6006 - learning_rate: 1.2500e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7141 - loss: 0.5923 - val_accuracy: 0.7085 - val_loss: 0.6002 - learning_rate: 1.2500e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.7154 - loss: 0.5889 - val_accuracy: 0.6944 - val_loss: 0.6044 - learning_rate: 1.2500e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7209 - loss: 0.5803 - val_accuracy: 0.7186 - val_loss: 0.5955 - learning_rate: 1.2500e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.7163 - loss: 0.5926 - val_accuracy: 0.7045 - val_loss: 0.5963 - learning_rate: 1.2500e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.7259 - loss: 0.5802 - val_accuracy: 0.7058 - val_loss: 0.5969 - learning_rate: 1.2500e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.7182 - loss: 0.5832 - val_accuracy: 0.7025 - val_loss: 0.5984 - learning_rate: 1.2500e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7126 - loss: 0.5873 - val_accuracy: 0.7085 - val_loss: 0.6009 - learning_rate: 1.2500e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m185/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7182 - loss: 0.5882\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - accuracy: 0.7182 - loss: 0.5881 - val_accuracy: 0.6985 - val_loss: 0.5983 - learning_rate: 1.2500e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7239 - loss: 0.5845 - val_accuracy: 0.7058 - val_loss: 0.5936 - learning_rate: 6.2500e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.7274 - loss: 0.5715 - val_accuracy: 0.7132 - val_loss: 0.5886 - learning_rate: 6.2500e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7200 - loss: 0.5772 - val_accuracy: 0.7139 - val_loss: 0.5906 - learning_rate: 6.2500e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7277 - loss: 0.5771 - val_accuracy: 0.7099 - val_loss: 0.5898 - learning_rate: 6.2500e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7245 - loss: 0.5746 - val_accuracy: 0.7112 - val_loss: 0.5915 - learning_rate: 6.2500e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7206 - loss: 0.5779 - val_accuracy: 0.7132 - val_loss: 0.5898 - learning_rate: 6.2500e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m185/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7328 - loss: 0.5688\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7326 - loss: 0.5689 - val_accuracy: 0.7065 - val_loss: 0.5912 - learning_rate: 6.2500e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7185 - loss: 0.5888 - val_accuracy: 0.7159 - val_loss: 0.5903 - learning_rate: 3.1250e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7182 - loss: 0.5789 - val_accuracy: 0.7085 - val_loss: 0.5906 - learning_rate: 3.1250e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7215 - loss: 0.5803 - val_accuracy: 0.7032 - val_loss: 0.5922 - learning_rate: 3.1250e-05\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained model to your desired path\n",
    "model.save(\"D:/depression_detect/models/audio_emotion_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7199 - loss: 0.5778\n",
      "Test Accuracy: 71.32%\n",
      "Test Loss: 0.5886\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "True: 0, Predicted: 0\n",
      "True: 0, Predicted: 0\n",
      "True: 0, Predicted: 0\n",
      "True: 0, Predicted: 0\n",
      "True: 0, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Convert softmax output to label index\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Print some predictions\n",
    "for i in range(5):\n",
    "    print(f\"True: {true_labels[i]}, Predicted: {predicted_labels[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio extracted successfully: D:/depression_detect/videos/audio.wav\n",
      "❌ API request failed: HTTPConnectionPool(host='127.0.0.1', port=5000): Max retries exceeded with url: /predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000029C5A91C250>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "❌ Failed to predict emotion.\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# ✅ Set FFmpeg path manually\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\ffmpeg\\bin\"\n",
    "\n",
    "# Paths\n",
    "VIDEO_PATH = \"D:/depression_detect/videos/videoplayback.mp4\"\n",
    "AUDIO_PATH = \"D:/depression_detect/videos/audio.wav\"\n",
    "\n",
    "# Convert Video to Audio\n",
    "def extract_audio(video_path, audio_path):\n",
    "    audio = AudioSegment.from_file(video_path, format=\"mp4\")\n",
    "    audio.export(audio_path, format=\"wav\")\n",
    "    print(\"✅ Audio extracted successfully:\", audio_path)\n",
    "\n",
    "# Extract MFCC features\n",
    "def extract_mfcc(audio_path, sr=22050, n_mfcc=13):\n",
    "    y, sr = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    if len(y) == 0:\n",
    "        raise ValueError(\"❌ Audio file is empty or corrupted.\")\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs, axis=1).tolist()\n",
    "\n",
    "# Send MFCC to voice model\n",
    "def predict_voice_emotion(mfcc_features):\n",
    "    url = \"http://127.0.0.1:5000/predict\"\n",
    "    data = {\"features\": mfcc_features}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result.get(\"prediction\", [None])[0]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"❌ API request failed:\", e)\n",
    "        return None\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    extract_audio(VIDEO_PATH, AUDIO_PATH)\n",
    "    mfcc_features = extract_mfcc(AUDIO_PATH)\n",
    "    \n",
    "    if mfcc_features:\n",
    "        voice_prediction = predict_voice_emotion(mfcc_features)\n",
    "        if voice_prediction is not None:\n",
    "            voice_emotion = \"Negative\" if voice_prediction == 0 else \"Positive\"\n",
    "            print(f\"🎙️ **Voice Emotion Prediction:** {voice_emotion}\")\n",
    "        else:\n",
    "            print(\"❌ Failed to predict emotion.\")\n",
    "    else:\n",
    "        print(\"❌ Failed to extract MFCC features.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "AudioSegment.ffmpeg = \"C:/ffmpeg/bin/ffmpeg.exe\"  # Set FFmpeg path manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"D:/depression_detection/models/audio_emotion_model.keras\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"D:/depression_detection/models/audio_emotion_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"D:/depression_detection/models/audio_emotion_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"D:/depression_detection/models/audio_emotion_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.7163 - loss: 0.5848 - val_accuracy: 0.7126 - val_loss: 0.5900 - learning_rate: 3.1250e-05\n",
      "Epoch 2/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7245 - loss: 0.5784 - val_accuracy: 0.7126 - val_loss: 0.5879 - learning_rate: 3.1250e-05\n",
      "Epoch 3/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7199 - loss: 0.5722 - val_accuracy: 0.7085 - val_loss: 0.5906 - learning_rate: 3.1250e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7213 - loss: 0.5738 - val_accuracy: 0.7132 - val_loss: 0.5886 - learning_rate: 3.1250e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7225 - loss: 0.5774 - val_accuracy: 0.7105 - val_loss: 0.5895 - learning_rate: 3.1250e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7158 - loss: 0.5801 - val_accuracy: 0.7139 - val_loss: 0.5877 - learning_rate: 3.1250e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7203 - loss: 0.5714 - val_accuracy: 0.7126 - val_loss: 0.5895 - learning_rate: 3.1250e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7258 - loss: 0.5729 - val_accuracy: 0.7166 - val_loss: 0.5867 - learning_rate: 3.1250e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7260 - loss: 0.5800 - val_accuracy: 0.7186 - val_loss: 0.5863 - learning_rate: 3.1250e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7189 - loss: 0.5801 - val_accuracy: 0.7119 - val_loss: 0.5866 - learning_rate: 3.1250e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7263 - loss: 0.5773 - val_accuracy: 0.7126 - val_loss: 0.5874 - learning_rate: 3.1250e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7156 - loss: 0.5770 - val_accuracy: 0.7132 - val_loss: 0.5863 - learning_rate: 3.1250e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7138 - loss: 0.5789 - val_accuracy: 0.7119 - val_loss: 0.5871 - learning_rate: 3.1250e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m185/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7137 - loss: 0.5848\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7138 - loss: 0.5847 - val_accuracy: 0.7119 - val_loss: 0.5863 - learning_rate: 3.1250e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7329 - loss: 0.5703 - val_accuracy: 0.7152 - val_loss: 0.5864 - learning_rate: 1.5625e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7144 - loss: 0.5792 - val_accuracy: 0.7132 - val_loss: 0.5863 - learning_rate: 1.5625e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7260 - loss: 0.5759 - val_accuracy: 0.7126 - val_loss: 0.5867 - learning_rate: 1.5625e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7166 - loss: 0.5797 - val_accuracy: 0.7139 - val_loss: 0.5857 - learning_rate: 1.5625e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7396 - loss: 0.5599 - val_accuracy: 0.7139 - val_loss: 0.5868 - learning_rate: 1.5625e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7149 - loss: 0.5726 - val_accuracy: 0.7146 - val_loss: 0.5862 - learning_rate: 1.5625e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7277 - loss: 0.5770 - val_accuracy: 0.7179 - val_loss: 0.5858 - learning_rate: 1.5625e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7317 - loss: 0.5620 - val_accuracy: 0.7166 - val_loss: 0.5855 - learning_rate: 1.5625e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7173 - loss: 0.5761 - val_accuracy: 0.7166 - val_loss: 0.5847 - learning_rate: 1.5625e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7195 - loss: 0.5756 - val_accuracy: 0.7146 - val_loss: 0.5851 - learning_rate: 1.5625e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7213 - loss: 0.5750 - val_accuracy: 0.7159 - val_loss: 0.5850 - learning_rate: 1.5625e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.7212 - loss: 0.5713 - val_accuracy: 0.7152 - val_loss: 0.5851 - learning_rate: 1.5625e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7097 - loss: 0.5854 - val_accuracy: 0.7139 - val_loss: 0.5853 - learning_rate: 1.5625e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m185/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7172 - loss: 0.5859\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7174 - loss: 0.5856 - val_accuracy: 0.7139 - val_loss: 0.5850 - learning_rate: 1.5625e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7220 - loss: 0.5725 - val_accuracy: 0.7152 - val_loss: 0.5854 - learning_rate: 7.8125e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7283 - loss: 0.5646 - val_accuracy: 0.7132 - val_loss: 0.5853 - learning_rate: 7.8125e-06\n",
      "Epoch 31/50\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.7253 - loss: 0.5645 - val_accuracy: 0.7173 - val_loss: 0.5851 - learning_rate: 7.8125e-06\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "history_csv_path = \"D:/depression_detect/results/audio_training_history.csv\"\n",
    "epoch_log_path = \"D:/depression_detect/results/audio_epoch_log.csv\"\n",
    "model_save_path = \"D:/depression_detect/models/audio_emotion_model.keras\"\n",
    "\n",
    "# Custom Callback for logging per epoch\n",
    "class EpochLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        df = pd.DataFrame([logs])\n",
    "        df[\"epoch\"] = epoch + 1\n",
    "        if not os.path.exists(epoch_log_path):\n",
    "            df.to_csv(epoch_log_path, index=False)\n",
    "        else:\n",
    "            df.to_csv(epoch_log_path, mode='a', header=False, index=False)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[lr_scheduler, early_stopping, EpochLogger()]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "\n",
    "# Save training history to CSV\n",
    "pd.DataFrame(history.history).to_csv(history_csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
